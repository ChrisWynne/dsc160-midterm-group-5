{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify API Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once\n",
    "#!pip install spotipy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credentials(api_cred_fp):\n",
    "    if os.path.exists(api_cred_fp):\n",
    "        with open(api_cred_fp) as json_file:\n",
    "            creds = json.load(json_file)\n",
    "        return creds\n",
    "    else:\n",
    "        \"Credentials File Not Found\"\n",
    "\n",
    "def load_cached_requests(fp,is_master):\n",
    "    if os.path.exists(fp):\n",
    "        with open(fp) as json_file:\n",
    "            ret_dic = json.load(json_file)\n",
    "            print(\"Loaded cached data at:\",fp)\n",
    "    else:\n",
    "        print(\"Failed to load cached data at:\",fp)\n",
    "        ret_dic = {}\n",
    "        if is_master:\n",
    "            ret_dic['master_tracks'] = {}\n",
    "            ret_dic['master_playlists'] = {}\n",
    "            ret_dic['master_audio_features'] = {}\n",
    "            ret_dic['playlist_tracks'] = {}\n",
    "            \n",
    "    return ret_dic\n",
    "\n",
    "def save_cached_requests(save_dic, save_dir,fname):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    with open(save_dir+fname,'w+') as outfile:\n",
    "        json.dump(save_dic,outfile)\n",
    "\n",
    "def save_csvs(df_list,dir_list,fname_list):\n",
    "    if len(df_list) == len(dir_list) == len(fname_list):\n",
    "        for i in range(len(df_list)):\n",
    "            cur_df = df_list[i]\n",
    "            cur_dir = dir_list[i]\n",
    "            cur_fname = fname_list[i]\n",
    "            if not os.path.exists(cur_dir):\n",
    "                os.makedirs(cur_dir)\n",
    "            cur_df.to_csv(cur_dir+cur_fname,index=False)\n",
    "            print(\"CSV Saved at:\",cur_dir+cur_fname)\n",
    "    else:\n",
    "        print(\"Save Failed: List lengths must be the same\")\n",
    "\n",
    "def save_csv(df, save_dir, fname):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    df.to_csv(save_dir + fname,index=False)\n",
    "    print(\"CSV Saved at:\",save_dir+fname)\n",
    "    \n",
    "def get_playlists_data_from_category(cat_id,master_playlist_dic,sp,playlist_tracks_dic,master_track_dic):\n",
    "    # api call\n",
    "    search_result = sp.category_playlists(category_id=cat_id,limit=50,country=\"US\")\n",
    "    playlist_ids = []\n",
    "    for playlist in search_result['playlists']['items']:\n",
    "        playlist_ids.append(playlist['id'])\n",
    "    playlists_data = []\n",
    "    # updates the playlist dic with info about playlist\n",
    "    for playlist_id in playlist_ids:\n",
    "        # avoids api call using cached request, could hold outdated info if playlist is changed recently\n",
    "        if playlist_id in master_playlist_dic.keys():\n",
    "            playlist_data = master_playlist_dic[playlist_id]\n",
    "        else:\n",
    "            # api call\n",
    "            playlist_data = sp.playlist(playlist_id)\n",
    "            # update master playlist dic with playlist data\n",
    "            master_playlist_dic[playlist_id] = playlist_data\n",
    "        playlists_data.append(playlist_data)\n",
    "    # updates the playlist track dic with all of the track ids in the playlist\n",
    "    all_playlists_tracks = []\n",
    "    for playlist_id in playlist_ids:\n",
    "        cur_playlist_tracks = get_full_playlist_tracks(playlist_tracks_dic, playlist_id, \n",
    "                                                       master_playlist_dic, master_track_dic,sp)\n",
    "        \n",
    "        all_playlists_tracks.extend(cur_playlist_tracks)\n",
    "    return all_playlists_tracks\n",
    "\n",
    "def get_track_data_from_playlists(genre_tracks,master_audio_features_dic,\n",
    "                                  sp,master_playlist_dic):\n",
    "    timed_out = False\n",
    "    ret_df = pd.DataFrame()\n",
    "    valid_genre_track_ids = []\n",
    "    for cur_track_id in genre_tracks:\n",
    "        if cur_track_id != None:\n",
    "            valid_genre_track_ids.append(cur_track_id)            \n",
    "                \n",
    "    # making a list of non cached track audio features\n",
    "    not_cached_ids = []\n",
    "    for track_id in valid_genre_track_ids:\n",
    "        if track_id not in master_audio_features_dic.keys():\n",
    "            not_cached_ids.append(track_id)\n",
    "\n",
    "    # get audio features of non cached tracks\n",
    "    if len(not_cached_ids) > 0:\n",
    "        split_not_cached_ids = [not_cached_ids[x:x+80] for x in range(0,len(not_cached_ids),80)]\n",
    "        not_cached_data = []\n",
    "        print(\"Making\",len(not_cached_ids),\"API Calls\")\n",
    "        for batch_ids in split_not_cached_ids:\n",
    "            try:\n",
    "                # api call\n",
    "                not_cached_data.extend(sp.audio_features(batch_ids))\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                print(\"Error getting audio features, retrying...\")\n",
    "                timed_out = True\n",
    "                time.sleep(10)\n",
    "            err_count = 0\n",
    "            while timed_out:\n",
    "                try:\n",
    "                    not_cached_data.extend(sp.audio_features(batch_ids))\n",
    "                    timed_out=False\n",
    "                except:\n",
    "                    if err_count > 5:\n",
    "                        print(\"Too many errors\")\n",
    "                        raise\n",
    "                    else:\n",
    "                        print(\"Error getting audio features, retrying...\")\n",
    "                        time.sleep(10*(err_count+1))\n",
    "                        err_count += 1\n",
    "        # caching the non cached track audio features\n",
    "        for i in range(len(not_cached_data)):\n",
    "            track_feats = not_cached_data[i]\n",
    "            if track_feats:\n",
    "                master_audio_features_dic[track_feats['id']] = track_feats\n",
    "            else:\n",
    "                master_audio_features_dic[not_cached_ids[i]] = None\n",
    "\n",
    "    # iterating current playlist tracks and accessing the cached audio features\n",
    "    playlist_track_feats = []\n",
    "    for track_id in valid_genre_track_ids:\n",
    "        cur_feats = master_audio_features_dic[track_id]\n",
    "        if cur_feats:\n",
    "            playlist_track_feats.append(cur_feats)\n",
    "\n",
    "    # building return df\n",
    "    genre_df = pd.DataFrame(playlist_track_feats)\n",
    "    return genre_df\n",
    "\n",
    "def main_scrape(cat_ids,csv_save_dir,csv_fname,cred_fp,cache_dir,all_requests_fname):\n",
    "    # initializing vars\n",
    "    creds = get_credentials(cred_fp)\n",
    "    client_id = creds['client_id']\n",
    "    client_secret = creds['client_secret']\n",
    "    client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "    sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "    all_requests_dic = load_cached_requests(cache_dir + all_requests_fname,is_master=True)\n",
    "    master_track_dic = all_requests_dic['master_tracks']\n",
    "    master_playlist_dic = all_requests_dic['master_playlists']\n",
    "    master_audio_features_dic = all_requests_dic['master_audio_features']\n",
    "    playlist_tracks_dic = all_requests_dic['playlist_tracks']\n",
    "    \n",
    "    # iterating through categories\n",
    "    master_df = pd.DataFrame()\n",
    "    for cat_id in cat_ids:\n",
    "        # gets playlist songs for top 50 playlists in each category\n",
    "        cat_tracks = get_playlists_data_from_category(cat_id,master_playlist_dic,sp,\n",
    "                                                      playlist_tracks_dic,master_track_dic)\n",
    "        \n",
    "        # gets audio features for all category songs\n",
    "        cat_df = get_track_data_from_playlists(cat_tracks,master_audio_features_dic,\n",
    "                                               sp,master_playlist_dic)\n",
    "        cat_df['genre(s)'] = cat_id\n",
    "        master_df = pd.concat([master_df,cat_df])\n",
    "        print(\"Caching \" + cat_id + \" requests...\")\n",
    "        # may not save new requests to the master cache dic, need to confirm/deny\n",
    "        save_cached_requests(all_requests_dic,cache_dir,all_requests_fname)\n",
    "        \n",
    "    # cleaning master df    \n",
    "    master_df = master_df.drop_duplicates(subset=['id','genre(s)'])\n",
    "    master_df = master_df.reset_index(drop=True)\n",
    "    master_df = master_df.drop(['type','track_href','uri'],axis=1)\n",
    "    master_df = get_track_metadata(master_df,master_track_dic).reset_index(drop=True)\n",
    "    \n",
    "    # write df to disk\n",
    "    save_csv(master_df,csv_save_dir,csv_fname)\n",
    "    return master_df\n",
    "\n",
    "def get_full_playlist_tracks(playlist_tracks_dic, cur_id, master_playlist_dic,master_track_dic,sp):\n",
    "    timed_out = False\n",
    "    if cur_id in playlist_tracks_dic.keys():\n",
    "        print(\"already cached playlist\",cur_id)\n",
    "    else:\n",
    "        time.sleep(.5)\n",
    "        print(\"Scraping playlist\",cur_id)\n",
    "        playlist_tracks_dic[cur_id] = []\n",
    "        # first run\n",
    "        cur_playlist_items = master_playlist_dic[cur_id]['tracks']['items']\n",
    "        cur_tracks = []\n",
    "        for cur_track in cur_playlist_items:\n",
    "            if cur_track and cur_track['track'] and cur_track['track']['id']:\n",
    "                cur_tracks.append(cur_track['track']['id'])\n",
    "                # updating master track dic for initial 100 songs\n",
    "                if cur_track['track']['id'] not in master_track_dic.keys():\n",
    "                    master_track_dic[cur_track['track']['id']] = cur_track\n",
    "        playlist_tracks_dic[cur_id].extend(cur_tracks)\n",
    "        # get entire playlist\n",
    "        total_songs = master_playlist_dic[cur_id]['tracks']['total']\n",
    "        cur_data = master_playlist_dic[cur_id]['tracks']\n",
    "        \n",
    "        # getting the rest of the track ids (init batch is only first 100 playlist songs)\n",
    "        while len(playlist_tracks_dic[cur_id]) < total_songs:  \n",
    "            # initial attempt\n",
    "            try:\n",
    "                next_data = sp.next(cur_data)\n",
    "            except:\n",
    "                print(\"Error getting playlist tracks, retrying...\")\n",
    "                timed_out = True\n",
    "                time.sleep(10)\n",
    "            # retries if initial attempt fails\n",
    "            err_count = 0\n",
    "            while timed_out:\n",
    "                try:\n",
    "                    next_data = sp.next(cur_data)\n",
    "                    timed_out=False\n",
    "                except:\n",
    "                    if err_count > 5:\n",
    "                        print(\"Too many failed attempts\")\n",
    "                        raise\n",
    "                    else:\n",
    "                        print(\"Error getting playlist tracks, retrying...\")\n",
    "                        time.sleep(10*(err_count+1))\n",
    "                        err_count += 1\n",
    "            cur_tracks = []\n",
    "            cur_playlist_items = cur_data['items']\n",
    "            for cur_track in cur_playlist_items:\n",
    "                if cur_track and cur_track['track'] and cur_track['track']['id']:\n",
    "                    cur_tracks.append(cur_track['track']['id'])\n",
    "                    # updating master track dic for latter 100+ songs\n",
    "                    if cur_track['track']['id'] not in master_track_dic.keys():\n",
    "                        master_track_dic[cur_track['track']['id']] = cur_track\n",
    "            playlist_tracks_dic[cur_id].extend(cur_tracks)         \n",
    "            cur_data = next_data\n",
    "            time.sleep(2)\n",
    "            print(\"Songs gathered:\",len(playlist_tracks_dic[cur_id]),\"of\",total_songs)\n",
    "    playlist_tracks_dic[cur_id] = list(set(playlist_tracks_dic[cur_id])) # remove duplicates\n",
    "    return playlist_tracks_dic[cur_id]\n",
    "\n",
    "def get_track_metadata(df,tracks_dic):\n",
    "    # getting metadata for each track\n",
    "    artist_ids = []\n",
    "    track_names = []\n",
    "    artist_names = []\n",
    "    for targ_track_id in df['id']: \n",
    "        if targ_track_id in tracks_dic.keys():\n",
    "            track_data = tracks_dic[targ_track_id]['track']\n",
    "            track_name = track_data['name']\n",
    "            track_names.append(track_name)\n",
    "            artists = track_data['artists']\n",
    "            artists_string = \"\"\n",
    "            artists_names_string = \"\"\n",
    "            for i in range(len(artists)):\n",
    "                if 'name' in artists[i].keys() and artists[i]['name']:\n",
    "                    if i == len(artists) - 1:\n",
    "                        artists_names_string += artists[i]['name']\n",
    "                    else:\n",
    "                        artists_names_string += artists[i]['name'] + \"//\"\n",
    "                if 'id' in artists[i].keys() and artists[i]['id']:\n",
    "                    if i == len(artists) - 1:\n",
    "                        artists_string += artists[i]['id']\n",
    "                    else:\n",
    "                        artists_string += artists[i]['id'] + \"//\"\n",
    "            artist_names.append(artists_names_string)\n",
    "            artist_ids.append(artists_string)\n",
    "        else:\n",
    "            artist_ids.append(np.nan)\n",
    "    df['artist_ids'] = artist_ids\n",
    "    df['artist_names'] = artist_names\n",
    "    df['name'] = track_names\n",
    "    # accounts for multiple genres (aka same song in different genre playlists)\n",
    "    df['genre(s)'] = df['id'].apply(lambda x:\"//\".join(df[df['id'] == x]['genre(s)'].values))\n",
    "    df = df.drop_duplicates(subset='id') # new duplicates created after combining genres\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['kpop','pop','rock','hiphop','reggae','jazz','rnb','classical','country','edm_dance']\n",
    "audio_features_dir = \"../data/test/all/metadata/\"\n",
    "audio_features_fname = \"all_audio_features.csv\"\n",
    "cred_fp = \"../api_cred.json\"\n",
    "cache_dir = \"../data/local/cached_requests/\"\n",
    "all_requests_fname = \"all_requests.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code requires a Spotify API client id and client secret, stored in a file called \"api_cred.json\" in the \n",
    "# root directory. Instructions for getting an API key here: \n",
    "# https://medium.com/@maxtingle/getting-started-with-spotifys-api-spotipy-197c3dc6353b\n",
    "#\n",
    "# When running this code, it will scrape around 23 thousand songs and take between 15 - 30 min.\n",
    "# For testing purposes, there is a pre-scraped dataset at ../data/test/all/metadata/all_audio_features.csv\n",
    "# If using the csv, just move on to the eda notebook as this notebook only scrapes the dataset.\n",
    "\n",
    "df = main_scrape(genres,audio_features_dir,audio_features_fname,cred_fp,cache_dir,all_requests_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
